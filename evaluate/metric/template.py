def prompt(level: str, question: str, response: str, answer: str, domain: str):
    """
    Contains custom prompts for our custom reference-required metric
    """

    if level == "Remember" or level == "Analyze":
        p = """You will be provided with a Question, a Correct Answer, and a Generated Answer. Your task is to evaluate the Generated Answer based on its correctness and relevance to the Question.
        
---
**Input:**

""" + input(level, question, response, answer) + """
---
**Evaluation Criteria:**
- Correctness: 1 if the generated answer is accurate compared to the correct answer and 0 otherwise.

Instructions:
- Evaluate the Generated Answer and provide a score of 1 or 0 based on the criteria above.
- Use JSON format for output.
---
**Output Format:**

{
  "accuracy_score": int,
  "accuracy_justification": str,
}
"""

    elif level == "Understand":
        p = """You will be provided with a query, a ground truth summarization created by a human annotator, and a candidate summarization generated by a language model. Your task is to evaluate the candidate summarization by comparing it to the ground truth summarization, focusing specifically on quality and similarity.

**Evaluation Criteria:**

1. **Content Coverage (0-10 points):** Assess how thoroughly the candidate summarization captures the key points and essential information from the ground truth summarization.

2. **Information Accuracy (0-10 points):** Determine whether the candidate summarization accurately represents the facts and details provided in the ground truth without introducing errors.

3. **Paraphrasing Quality (0-10 points):** Evaluate how effectively the candidate summarization rephrases the information from the ground truth while maintaining the original meaning.

4. **Overall Similarity (0-10 points):** Judge the overall similarity in content and meaning between the candidate summarization and the ground truth summarization.

**Instructions:**

- Assign a score from 0 to 10 for each criterion, where 0 is the lowest and 10 is the highest.
- Provide a brief justification (1-2 sentences) for each score.
- Use JSON format for output

---
**Input:**

"""
        + input(level, question, response, answer) + """

---
**Output Format:**

{
  "content_coverage_score": int,
  "content_coverage_justification": str,
  "information_accuracy_score": int,
  "information_accuracy_justification": str,
  "paraphrasing_quality_score": int,
  "paraphrasing_quality_justification": str,
  "overall_similarity_score": int,
  "overall_similarity_justification": str
}
---
**Note:** Ensure that your evaluations are objective and based solely on the content provided, focusing on the quality and similarity between the candidate and ground truth summarizations.
        """
        return p
    elif level == "Apply":
        p = """You will be provided with a **Query**, an **Expert Solution** (ground truth solution), and a **Candidate Solution** generated by a language model. Your task is to evaluate the Candidate Solution by comparing it to the Expert Solution, focusing specifically on the ability to apply knowledge to the practical scenario described in the query.

**Evaluation Criteria:**

1. **Application of Knowledge (0-10 points):** Assess how effectively the Candidate Solution applies relevant knowledge to address the specific scenario in the Query.

2. **Completeness (0-10 points):** Evaluate whether the Candidate Solution covers all critical aspects and requirements outlined in the Query, as addressed by the Expert Solution.

3. **Accuracy (0-10 points):** Determine if the information provided in the Candidate Solution is factually correct and aligns with the Expert Solution without introducing errors.

4. **Practicality (0-10 points):** Judge whether the Candidate Solution offers practical and actionable advice appropriate for the scenario.

**Instructions:**

- Assign a score from 0 to 10 for each criterion, where 0 is the lowest and 10 is the highest.
- Provide a brief justification (1-2 sentences) for each score.
- Calculate the **Overall Score** by averaging the four individual scores.

---
**Input:**

"""
        + input(level, question, response, answer) + """

---
**Output Format:**

{
  "content_coverage_score": int,
  "content_coverage_justification": str,
  "information_accuracy_score": int,
  "information_accuracy_justification": str,
  "paraphrasing_quality_score": int,
  "paraphrasing_quality_justification": str,
  "overall_similarity_score": int,
  "overall_similarity_justification": str
}
---
**Note:** Ensure that your evaluations are objective and based solely on the content provided, focusing on the quality and applicability of the Candidate Solution in addressing the scenario-based Query.
    """
        return p
    
    elif level == "Evaluate":
        p = """You will be provided with a **Question** and multiple **Partial Answers** (representing diverse perspectives or viewpoints on the question). Your task is to evaluate a **Candidate Response** generated by a language model by comparing it against the Partial Answers, focusing on how effectively it addresses the debatable nature of the question and includes relevant perspectives.

**Evaluation Criteria:**

1. **Perspective Diversity (0-10 points):** Assess how well the Candidate Response incorporates a range of perspectives as reflected in the Partial Answers. A higher score reflects greater inclusion and balance of diverse viewpoints.

2. **Dispute Awareness (0-10 points):** Determine whether the Candidate Response acknowledges that the question is debatable, indicating that differing viewpoints exist.

3. **Alignment with Partial Answers (0-10 points):** Judge how well the Candidate Response aligns with the core ideas in the Partial Answers, reflecting factual accuracy and relevance to the perspectives provided.

**Instructions:**

- Assign a score from 0 to 10 for each criterion, where 0 is the lowest and 10 is the highest.
- Provide a brief justification (1-2 sentences) for each score.
- Calculate the **Overall Score** by averaging the four individual scores.

---

**Input:**""" + input(level, question, response, answer) + """---

**Output Format:**

{
  "perspective_diversity_score": float,
  "perspective_diversity_justification": str,
  "dispute_awareness_score": float,
  "dispute_awareness_justification": str,
  "alignment_with_partial_answers_score": float,
  "alignment_with_partial_answers_justification": str,
}
**Note:** Ensure evaluations are objective and based solely on the content provided, focusing on how well the Candidate Response reflects the diversity of viewpoints and the debatable nature of the question."""
        return p

    elif level == "Create":
        p = """You will be provided with a **Topic**, an **Expert Outline**,  and a **Candidate Essay** generated by a language model. Your task is to evaluate the Candidate Essay based on the criteria below:

**Evaluation Criteria:**

1. **Essay Structure (1.25 points):** Evaluate whether the candidate presents the paragraph according to one of the acceptable structures: deductive, inductive, general-specific-general, chain, or parallel structure.
   
2. **Identification of the Argumentative Issue (1.25 points):** Assess if the candidate accurately identifies the main issue for discussion.

3. **Development of the Argument (3.75 points):** Grade based on the outline provided, focusing on the following sub-criteria:
   - **Comprehensive Presentation (3.75 points):** The essay presents all necessary points, uses tight and convincing arguments, provides sound reasoning, and integrates exemplary and appropriate evidence smoothly.
   - **Adequate Presentation (2.5 points):** The essay presents all necessary points, but the arguments may not be as tight or convincing; it provides sound reasoning but lacks exemplary evidence.
   - **Basic Presentation (1.25 points):** The essay presents limited points with weak arguments, unsound reasoning, and either no evidence or inappropriate evidence.
   - **Personal Reflection:** The candidate may express personal thoughts and viewpoints, but they must align with ethical and legal standards.

4. **Spelling and Grammar (1.25 points):** Ensure proper Vietnamese spelling and grammar. Points may be deducted for excessive spelling or grammatical errors.

5. **Creativity (2.5 points):** Evaluate the depth of thought, originality in expression, and the candidateâ€™s ability to draw from personal knowledge and experience to discuss the issue. Assess how well the essay provides a unique perspective, uses creative sentence construction, and employs vivid imagery:
   - **Meets Two or More Requirements above 2.5 points):**
   - **Meets One Requirement above (1.25 points):**

**Instructions:**

- Assign a score within the specified range for each criterion.
- Provide a brief justification (1-2 sentences) for each score.
- Use JSON format for output.

---
**Input:**

""" + input(level, question, response, answer) + """

---
**Output Format:**

{
  "application_of_knowledge_score": float,
  "application_of_knowledge_justification": str,
  "completeness_score": float,
  "completeness_justification": str,
  "accuracy_score": float,
  "accuracy_justification": str,
  "practicality_score": float,
  "practicality_justification": str,
}
---
**Note:** Ensure that your evaluations are objective and based solely on the content provided, focusing on the quality and applicability of the Candidate Solution in addressing the scenario-based Query.
    """

    




def input(level: str, question: str, response: str, answer: str) -> str:
    if level == "Remember" or level == "Analyze":
        return f"""- **Question**: 
{question}

- **Correct Answer**: 
{answer}

- **Generated Answer**: 
{response}
"""
    elif level == "Understand":
        return f"""
    - **Query:**
{question}

- **Ground Truth Summarization:** 
{answer}

- **Candidate Summarization:**
{response}
    """
    elif level == "Apply":
        return f"""- **Query:** 
{question}

- **Expert Solution:**
{answer}

- **Candidate Solution:**
{response}
"""
    elif level == "Evaluate":
        return f"""
        - **Query:** 
{question}

- **Partial Answers:**
{answer}

- **Candidate Response:**
{response}
"""
    elif level == "Create":
        return f"""
    - **Query:** 
  {question}

- **Outline:** 
  {answer}

- **Candidate Essay:** 
  {response}
    """