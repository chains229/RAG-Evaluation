from pydantic import BaseModel
import google.generativeai as genai
import instructor
from deepeval.models import DeepEvalBaseLLM

class CustomGemini(DeepEvalBaseLLM):
    """
    Define Gemini as a Judge for deepeval
    """
    def __init__(self):
        self.model = genai.GenerativeModel("models/gemini-1.5-pro-002")

    def load_model(self):
        return self.model

    def generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        client = self.load_model()
        model_config = {"temperature": 1.0}
        instructor_client = instructor.from_gemini(
            client=client,
            mode=instructor.Mode.GEMINI_JSON,
            generation_config=model_config,
        )
        resp = instructor_client.messages.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            response_model=schema,
        )
        return resp

    async def a_generate(self, prompt: str, schema: BaseModel) -> BaseModel:
        return self.generate(prompt, schema)

    def get_model_name(self):
        return "Gemini 1.5"

def prompt(level: str, question: str, response: str, answer: str):
    """
    Contains custom prompts for our custom reference-required metric
    """

    if level == "Remember":
        ""

    elif level == "Understand":
        p = """You will be provided with a query, a ground truth summarization created by a human annotator, and a candidate summarization generated by a language model. Your task is to evaluate the candidate summarization by comparing it to the ground truth summarization, focusing specifically on quality and similarity.

**Evaluation Criteria:**

1. **Content Coverage (0-10 points):** Assess how thoroughly the candidate summarization captures the key points and essential information from the ground truth summarization.

2. **Information Accuracy (0-10 points):** Determine whether the candidate summarization accurately represents the facts and details provided in the ground truth without introducing errors.

3. **Paraphrasing Quality (0-10 points):** Evaluate how effectively the candidate summarization rephrases the information from the ground truth while maintaining the original meaning.

4. **Overall Similarity (0-10 points):** Judge the overall similarity in content and meaning between the candidate summarization and the ground truth summarization.

**Instructions:**

- Assign a score from 0 to 10 for each criterion, where 0 is the lowest and 10 is the highest.
- Provide a brief justification (1-2 sentences) for each score.
- Use JSON format for output

---
**Input:**

"""
        + input(level, question, response, answer) + """

---
**Output Format:**

{
  "content_coverage_score": int,
  "content_coverage_justification": str,
  "information_accuracy_score": int,
  "information_accuracy_justification": str,
  "paraphrasing_quality_score": int,
  "paraphrasing_quality_justification": str,
  "overall_similarity_score": int,
  "overall_similarity_justification": str
}
---
**Note:** Ensure that your evaluations are objective and based solely on the content provided, focusing on the quality and similarity between the candidate and ground truth summarizations.
        """
        return p
    elif level == "Apply":
        p = """You will be provided with a **Query**, an **Expert Solution** (ground truth solution), and a **Candidate Solution** generated by a language model. Your task is to evaluate the Candidate Solution by comparing it to the Expert Solution, focusing specifically on the ability to apply knowledge to the practical scenario described in the query.

**Evaluation Criteria:**

1. **Application of Knowledge (0-10 points):** Assess how effectively the Candidate Solution applies relevant knowledge to address the specific scenario in the Query.

2. **Completeness (0-10 points):** Evaluate whether the Candidate Solution covers all critical aspects and requirements outlined in the Query, as addressed by the Expert Solution.

3. **Accuracy (0-10 points):** Determine if the information provided in the Candidate Solution is factually correct and aligns with the Expert Solution without introducing errors.

4. **Practicality (0-10 points):** Judge whether the Candidate Solution offers practical and actionable advice appropriate for the scenario.

**Instructions:**

- Assign a score from 0 to 10 for each criterion, where 0 is the lowest and 10 is the highest.
- Provide a brief justification (1-2 sentences) for each score.
- Calculate the **Overall Score** by averaging the four individual scores.

---
**Input:**

"""
        + input(level, question, response, answer) + """

---
**Output Format:**

{
  "content_coverage_score": int,
  "content_coverage_justification": str,
  "information_accuracy_score": int,
  "information_accuracy_justification": str,
  "paraphrasing_quality_score": int,
  "paraphrasing_quality_justification": str,
  "overall_similarity_score": int,
  "overall_similarity_justification": str
}
---
**Note:** Ensure that your evaluations are objective and based solely on the content provided, focusing on the quality and applicability of the Candidate Solution in addressing the scenario-based Query.
    """
        return p
    elif level == "Analyze":
        ""
    elif level == "Evaluate":
        ""
    elif level == "Create":
        ""

    




def input(level: str, question: str, response: str, answer: str) -> str:
    

        return f"""
    - **Query:**
{question}

- **Ground Truth Summarization:** 
{answer}

- **Candidate Summarization:**
{response}
    """

def eval_steps(level: str):
    """
    Contains the evaluation steps for each level. This will be used for reference-required metrics
    """
    if level == "Remember":
        return ["Check if the generated answer is correct and directly relevant to the user query"]
    elif level == "Understand":
        return ["Assess how thoroughly the candidate summarization captures the key points and essential information from the ground truth summarization",
                "Determine whether the candidate summarization accurately represents the facts and details provided in the ground truth without introducing errors",
                "Evaluate how effectively the candidate summarization rephrases the information from the ground truth while maintaining the original meaning",
                "Judge the overall similarity in content and meaning between the candidate summarization and the ground truth summarization"
        ]
    elif level == "Apply":
        return ["Assess how effectively the Candidate Solution applies relevant knowledge to address the specific scenario in the Query",
                "Evaluate whether the Candidate Solution covers all critical aspects and requirements outlined in the Query, as addressed by the Expert Solution",
                "Determine if the information provided in the Candidate Solution is factually correct and aligns with the Expert Solution without introducing errors",
                "Judge whether the Candidate Solution offers practical and actionable advice appropriate for the scenario"
        ]
    elif level == "Analyze":
        return ["Assess how well the Candidate Response incorporates a range of perspectives as reflected in the Partial Answers.",
                "Determine whether the Candidate Response acknowledges that the question is debatable, indicating that differing viewpoints exist",
                "Judge how well the Candidate Response aligns with the core ideas in the Partial Answers, reflecting factual accuracy and relevance to the perspectives provided"
        ]
    elif level == "Evaluate":
        return ["Assess how well the Candidate Response incorporates a range of perspectives as reflected in the Partial Answers.",
                "Determine whether the Candidate Response acknowledges that the question is debatable, indicating that differing viewpoints exist",
                "Judge how well the Candidate Response aligns with the core ideas in the Partial Answers, reflecting factual accuracy and relevance to the perspectives provided"
        ]
    else:
        return ["Evaluate whether the candidate presents the paragraph according to one of the acceptable structures: deductive, inductive, general-specific-general, chain, or parallel structure.",
                "Assess if the candidate accurately identifies the main issue for discussion",
                "Ensure proper Vietnamese spelling and grammar. Points may be deducted for excessive spelling or grammatical errors.",
                "Evaluate the depth of thought, originality in expression, and the candidateâ€™s ability to draw from personal knowledge and experience to discuss the issue"
        ]
    
